% This file is part of the Radio Array Model project.
% Copyright 2012 David W. Hogg

\documentclass[12pt]{article}

\newcounter{hogg}
\setcounter{hogg}{1}
\newcommand{\hoggitem}{\textsl{(\thehogg)}\stepcounter{hogg}}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\CLEAN}{\project{CLEAN}}
\newcommand{\NAELC}{\project{NAELC}}

\renewcommand{\exp}[1]{e^{#1}}
\newcommand{\set}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\expectation}[1]{\tilde{#1}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\normal}{N}
\newcommand{\Normal}{\normal}
\newcommand{\transpose}[1]{{#1}^{\!\mathsf{T}}}

\begin{document}

\textsl{This document has been written by David~W.~Hogg (NYU).  It is
  based in part on conversations in Valchava on 2012-08-16 among
  Fergus, Harmeling, Hirsch, Hogg, Muandet, Rix, Sch\"olkopf, Schuler
  and in part on coversations in Heidelberg on 2012-08-08 among
  Bigiel, Herbst, Hogg, Rix, Walter.}

\paragraph{Introduction}

...Interferometry data have these properties...positive reasons to do
what we are doing...

...Define visibility, clean beam, dirty beam; also units and
dimensions...

...For reconstructing a ``best'' image from radio interferometry data,
he radio world uses \CLEAN--—and an infinite variety of alternatives
and modifications thereto—--for building radio interferometry maps...

\paragraph{What's wrong with \CLEAN?}

There are many reasons traditional \CLEAN\ needs replacement or
improvement.  Some of these issues have been addressed in small
modifications to the traditional method, leading to a zoo of
baby-\CLEAN s.  The comments below apply only to the most vanilla or
traditional implementations of \CLEAN, though we know of no widely
used system that addresses all of these.

\hoggitem~\CLEAN\ is some kind of optimizer, but a heuristic
optimizer.  That is, there are optimizers out there with better
properties.  \hoggitem~It is also not optimizing a well-specified (let
alone justified) scalar objective.  Fergus said ``a conservative
greedy algorithm'' but with an unknown objective.  Related to this,
\hoggitem~it requires heuristic decision-making about stopping.  There
is a sense of things being over-cleaned or under.

\hoggitem~The noise in the final map \CLEAN\ makes can only be
estimated by measuring variance-like statistics in empty regions.  It
has no error or noise model and doesn't propagate uncertainties from
the fundamental visibilities (let alone calibration uncertainties and
so on).  This blindness to likelihood or noise leads to the problem
that \hoggitem~\CLEAN can provide no mechanism for producing or
quantitatively comparing alternative maps.  It produces only a single
point estimate and no judgement of that estimate or sampling around
that estimate.  It is a procedure rather than a probabilistic
framework.  \hoggitem~There is also no mechanism by which the map it
produces is standardly or by convention projected back into the space
of visibilities and compared to the raw data to vet or confirm any
fundamental or even heuristic noise model, or to judge the
significance of faint sources in the maps it produces.

The final map made by \CLEAN\ is a sum of the clean flux (convolved
with the clean beam) and the residual flux (which is effectively
convolved with the dirty beam).  Therefore, \hoggitem~the final map it
makes has no consistent point-spread function: Some of the flux is
convolved with the clean beam, and some is convolved with the dirty
beam.  This means that there is no well-defined point-spread function
and, perhaps more importantly \hoggitem~the absolute calibration of
the map is ill-defined.  The units of the map are ``Jy per clean
beam'' for the bright parts and ``Jy per dirty beam'' for the faint
parts.

On a more technical level (but equally concerning), \hoggitem~\CLEAN
requires binning of the visibility data in the $u$--$v$ plane
(presumably for fastness).  Rix teaches us that ``binning is
sinning''.

\paragraph{Radio interferometric likelihood}

We get from the telescope \emph{visibilities}, which for our purposes
are $N$ samples $d_n$ of the form
\begin{eqnarray}
d_n &\equiv& \set{a_n, b_n, \nu_n, t_n, u_n, v_n, w_n, Z_n}
\\
Z_n &\equiv& A_n + i\,B_n
\\
Z_n &\equiv& \left|Z_n\right|\,\exp{i\,\phi_n}
\quad,
\end{eqnarray}
where $a_n$ and $b_n$ are the identities of the two antennae used to
make the baseline used for sample $n$, $\nu_n$ is the electromagnetic
frequency channel of the observation, $t_n$ is the time at which the
sample was taken, $(u_n, v_n)$ is the $u$--$v$-plane location of the
sample, $w_n$ can be ignored (for our purposes), and $Z_n$ is the
measured complex amplitude or \emph{visibility}.  In what follows, we
will assume that every aspect of the data is correctly calibrated.
Under this assumption, the $Z_n$ are really the data; the $(u_n,
v_n)$ and $\nu_n$ can be thought of as prior information.

Each sample comes from a baseline connecting two antennae; each
antenna has a beam pattern on the sky---what would be the point-spread
function for a single-dish observation---and a system temperature.
There is also a correlator temperature of some kind, so we expect a
noise variance $\sigma_n^2$ on the complex $Z_n$
\begin{eqnarray}
\sigma_n^2 &\propto& T_a + T_b + T_{ab}
\quad ,
\end{eqnarray}
where $T_a$ is the noise temperature of antenna $a_n$, $T_b$ is the
noise temperature of antenna $b_n$, and $T_{ab}$ is an additional
correlator noise for this baseline.

The generative model for our purposes is as follows: There is a
time-independent intensity field $I(x,y\given\nu)$ as a function of
electromagnetic frequency $\nu$, defined in sky coordinates $(x,y)$ or
the $x$--$y$ plane.  At each $(u_n, v_n)$ sample location in the
$u$--$v$ plane the expected complex visibility is
\begin{eqnarray}
\expectation{Z}_n &=& \int \psi_a(x,y\given\nu_n)\,\psi_b(x,y\given\nu_n)\,I(x,y\given\nu_n)
                         \,\exp{-2\pi\,i\,[u_n\,x + v_n\,y]}\,\dd x\,\dd y
\quad ,
\end{eqnarray}
(HOGG: work out pre-factors like $\sqrt{\pi}$ etc.) where
$\psi_a(x,y\given\nu_n)$ is the single-antenna beam pattern for
antenna $a$ at frequency $\nu_n$, $\psi_b$ is the same for antenna
$b$, and we have assumed that the electromagnetic frequency bandpass
$\Delta\nu_n$ is small.

If the intensity field $I(x,y\given\nu)$ is governed by (a vector,
list, or blob of) parameters $\theta$, then under the Gaussian-noise
assumption there is a likelihood for those parameters
\begin{eqnarray}
p(Z_n|\theta) &=& \normal(Z_n\given\expectation{Z}_n,\sigma_n^2)
\\
-2\,\ln p(Z_n|\theta) &=& \frac{[Z_n - \expectation{Z}_n]^2}{\sigma_n^2}
\\
-2\,\ln p(D|\theta) &=& \sum_{n=1}^N \frac{[Z_n - \expectation{Z}_n]^2}{\sigma_n^2}
\\
D &\equiv& \set{Z_n}_{n=1}^N
\quad ,
\end{eqnarray}
where $\normal(x\given m,V)$ is the Gaussian distribution for $x$
given mean $m$ and variance $V$, we have ignored constant offsets in
the logarithms, we have assembled all the $Z_n$ into a large data blob
$D$, and we have assumed that the noise contributions to the $Z_n$ are
independent.  The likelihood is effectively conditioned on an
assumption that $(u_n, v_n)$ and $\nu_n$---and our knowledge about
beam patterns and noise and so on---are reliable.

\paragraph{Intensity models}

There are two general approaches to modeling the intensity field.  In
the first---the \emph{catalog approach}---the intensity is modeled as
being generated by a finite number of discrete objects, each of which
has a flux, a celestial position, and some (perhaps trivial)
morphology.  In this approach, all of the object properties are seen
as free parameters, along with (usually) the total number of objects
in the catalog.  In the second---the \emph{raster approach}---the
intensity is modeled by a set of (perhaps trivial) spatially compact
basis functions arranged in a pre-defined grid in celestial
coordinates.  In this approach, the only parameters permitted to vary
are the amplitudes---of the basis functions.  In addition we will
consider a \emph{hybrid approach} in which the intensity is modeled as
a sum or mixture of a catalog and a raster, with the catalog taking
care of the bright, compact sources, and the raster taking care of the
low-signal-to-noise and extended intensity.  (In some sense, The
traditional \CLEAN\ procedure produces a hybrid description of the
intensity, because the bright sources are ``cleaned'' as point sources
and the faint sources are left behind in the dirty residual expressed
as a raster image in the intensity domain.)

The catalog model makes the intensity a mixture of $M$ objects
\begin{eqnarray}
I(x,y\given\nu) &=& \sum_{m=1}^M f_m\,g(x,y\given s_m)
\\
\theta &\equiv& \set{f_m, s_m}_{m=1}^M
\quad,
\end{eqnarray}
where the $f_m$ are object fluxes, the function $g(x,y\given s)$ is a
function that can generate a family of sources with different
positions (and shapes), and the $s_m$ are parameter blobs containing
object positions, shapes, and orientations.  All the fluxes $f_m$ and
positions and shapes $s_m$ are variable parameters.  The catalog model
is not a general model---it is limited by the number of objects and
their morphological flexibility---but it is often fast to optimize and
well-matched to the scientific goals of an imaging project.
Optimization will not be convex, in general, because the positional
and shape parameters will usually affect the intensity in a non-linear
way.

The simplest catalog model is a \emph{mixture of delta-functions}, or
\begin{eqnarray}
g(x,y\given s_m) &=& \delta([x-x_m], [y-y_m])
\\
s_m &\equiv& \set{x_m, y_m}
\quad,
\end{eqnarray}
where $\delta()$ is the two-dimensional delta function and $s_m$
contains only positional information.  Another simple model (really a
generalization) is the \emph{mixture of Gaussians}, or
\begin{eqnarray}
g(x,y\given s_m) &=& \normal(\xi\given\mu_m,V_m)
\\
\xi &\equiv& \transpose{[x, y]}
\\
\mu_m &\equiv& \transpose{[x_m, y_m]}
\\
s_m &\equiv& \set{x_m, y_m, V_m}
\quad,
\end{eqnarray}
where $\normal()$ is here the two-dimensional gaussian, $\xi$ and
$\mu_m$ are two-dimensional position vectors, $V_m$ is a $2\times 2$
symmetric positive-definite variance tensor, and $s_m$ thereby
contains both positional and shape information.

The raster model also makes the intensity a mixture of basis
functions, but now we think of having many functions, all identical
except for positional shifts, and on a fixed grid;
\begin{eqnarray}
I(x,y\given\nu) &=& \sum_{k=1}^K a_k\,h_k(x, y)
\\
h_k(x, y) &\equiv& h([x-x_k], [y-y_k])
\\
\theta &\equiv& \set{a_k}_{k=1}^K
\quad,
\end{eqnarray}
where the $a_k$ are raster-element (``pixel'') intensities.  All the
intensities $a_k$ are variable parameters but the element positions
$(x_k, y_k)$ are seen as prior information.  The raster model is
``non-parametric'' in that it can represent very strange intensity
fields, but at the cost of large numbers of parameters.  One can hope
to keep things convex.

Again, the simplest pixel functions are delta-functions
\begin{eqnarray}
h_k(x,y) &=& \delta([x-x_k], [y-y_k])
\quad,
\end{eqnarray}
but there are many other options, including square top-hat functions
and circular Gaussians (HOGG put in formulae).

The hybrid method is more general, being a mixture of the two:
\begin{eqnarray}
I(x,y\given\nu) &=& \sum_{m=1}^M f_m\,g(x,y\given s_m)
                  + \sum_{k=1}^K a_k\,h_k(x, y)
\\
\theta &\equiv& \set{\set{f_m, s_m}_{m=1}^M, \set{a_k}_{k=1}^K}
\quad.
\end{eqnarray}
Optimization will be difficult because it has all the parameters of
both models, and all the non-convexity of the catalog method.  Because
traditional \CLEAN\ works by modeling parts of the intensity field as
point sources, and parts as a dirty residual map, it produces, in some
sense, a hybrid model of the intensity field.  It is not, however, an
optimal map in any sense, and it does not have consistent beam-shape
(PSF) properties, as noted above.

\paragraph{Intensity priors or regularization}

For any model of the intensity field---catalog, raster, or
hybrid---there is a large parameter vector $\theta$.  In general we
have \emph{priors} on this parameter vector that indicate the behavior
we would like for our inference in the absence of data or when the
data are not informative.  Because we are producing point estimates,
these things we call ``priors'' really involve not just our prior
expectations about the plausibility of any scene but also our
\emph{utility} or preferences.

...Harmeling thinks this might not be necessary.  Fergus and Schuler
think this is absolutely necessary...

...non-negativity...

...spike and slab...

...smoothness or gradients...

\paragraph{Initialization and optimization}

...Schuler will save us all?...

...Do we have tricks that make our inferences fast?  Certainly we know
that, for the raster components, the major operations are linear
operations and contain some identical linear-algebra objects at every
step...

\end{document}
