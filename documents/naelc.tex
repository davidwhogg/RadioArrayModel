% This file is part of the Radio Array Model project.
% Copyright 2012 David W. Hogg

\documentclass[12pt]{article}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\CLEAN}{\project{CLEAN}}
\newcommand{\NAELC}{\project{NAELC}}

\newcommand{\set}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\expectation}[1]{\tilde{#1}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\normal}{N}

\begin{document}

\textsl{This document has been written by David~W.~Hogg (NYU).  It is
  based in part on conversations in Valchava on 2012-08-16 among
  Fergus, Harmeling, Hirsch, Hogg, Muandet, Rix, Sch\"olkopf, Schuler
  and in part on coversations in Heidelberg on 2012-08-08 among
  Bigiel, Herbst, Hogg, Rix, Walter.}

...Interferometry data have these properties...positive reasons to do
what we are doing...

...\CLEAN\ is very wrong...negative reasons to do what we are doing...

\paragraph{Radio interferometric likelihood}

We get from the telescope \emph{visibilities}, which for our purposes
are $N$ samples $d_n$ of the form
\begin{eqnarray}
d_n &\equiv& \set{a_n, b_n, \nu_n, t_n, u_n, v_n, w_n, Z_n}
\\
Z_n &\equiv& A_n + i\,B_n
\\
Z_n &\equiv& \left|Z_n\right|\,\exp(i\,\phi_n)
\quad,
\end{eqnarray}
where $a_n$ and $b_n$ are the identities of the two antennae used to
make the baseline used for sample $n$, $\nu_n$ is the electromagnetic
frequency channel of the observation, $t_n$ is the time at which the
sample was taken, $(u_n, v_n)$ is the $u$--$v$-plane location of the
sample, $w_n$ can be ignored (for our purposes), and $Z_n$ is the
measured complex amplitude or \emph{visibility}.  In what follows, we
will assume that every aspect of the data is correctly calibrated.
Under this assumption, the $Z_n$ are really the data; the $(u_n,
v_n)$ and $\nu_n$ can be thought of as prior information.

Each sample comes from a baseline connecting two antennae; each
antenna has a system temperature, and there is also a correlator
temperature of some kind, so we expect a noise variance $\sigma_n^2$
on the complex $Z_n$
\begin{eqnarray}
\sigma_n^2 &\propto& T_a + T_b + T_{ab}
\quad ,
\end{eqnarray}
where $T_a$ is the noise temperature of antenna $a_n$, $T_b$ is the
noise temperature of antenna $b_n$, and $T_{ab}$ is an additional
correlator noise for this baseline.

The generative model for our purposes is as follows: There is a
time-independent intensity field $I(x,y\given\nu)$ as a function of
electromagnetic frequency $\nu$, defined in sky coordinates $(x,y)$ or
the $x$--$y$ plane.  At each $(u_n, v_n)$ sample location in the
$u$--$v$ plane the expected complex visibility is
\begin{eqnarray}
\expectation{Z}_n &=& \int I(x,y\given\nu_n)\,\exp(-2\pi\,i\,[u_n\,x + v_n\,y])\,\dd x\,\dd y
\quad ,
\end{eqnarray}
(HOGG: work out pre-factors like $\sqrt{\pi}$ etc.) where we have
assumed that the electromagnetic frequency bandpass $\Delta\nu_n$ is
small.

If the intensity field $I(x,y\given\nu)$ is governed by (a vector,
list, or blob of) parameters $\theta$, then under the Gaussian-noise
assumption there is a likelihood for those parameters
\begin{eqnarray}
p(Z_n|\theta) &=& \normal(Z_n\given\expectation{Z}_n,\sigma_n^2)
\\
-2\,\ln p(Z_n|\theta) &=& \frac{[Z_n - \expectation{Z}_n]^2}{\sigma_n^2}
\\
-2\,\ln p(D|\theta) &=& \sum_{n=1}^N \frac{[Z_n - \expectation{Z}_n]^2}{\sigma_n^2}
\\
D &\equiv& \set{Z_n}_{n=1}^N
\quad ,
\end{eqnarray}
where $\normal(x\given m,V)$ is the Gaussian distribution for $x$
given mean $m$ and variance $V$, we have ignored constant offsets in
the logarithms, we have assembled all the $Z_n$ into a large data blob
$D$, and we have assumed that the noise contributions to the $Z_n$ are
independent.  The likelihood is effectively conditioned on an
assumption that $(u_n, v_n)$ and $\nu_n$ are reliable.

\paragraph{Intensity models}

There are two general approaches to modeling the intensity field.  In
the first---the \emph{catalog approach}---the intensity is modeled as
being generated by a finite number of discrete objects, each of which
has a flux, a celestial position, and some (perhaps trivial)
morphology.  In this approach, all of the object properties are seen
as free parameters, along with (usually) the total number of objects
in the catalog.  In the second---the \emph{raster approach}---the
intensity is modeled by a set of (perhaps trivial) spatially compact
basis functions arranged in a pre-defined grid in celestial
coordinates.  In this approach, the only parameters permitted to vary
are the amplitudes---of the basis functions.  In addition we will
consider a \emph{hybrid approach} in which the intensity is modeled as
a sum or mixture of a catalog and a raster, with the catalog taking
care of the bright, compact sources, and the raster taking care of the
low-signal-to-noise and extended intensity.  (In some sense, The
traditional \CLEAN\ procedure produces a hybrid description of the
intensity, because the bright sources are ``cleaned'' as point sources
and the faint sources are left behind in the dirty residual expressed
as a raster image in the intensity domain.)

catalog:
\begin{eqnarray}
I(x,y\given\nu) &=& \sum_{m=1}^M f_m\,g(x,y\given s_m)
\\
\theta &\equiv& \set{f_m, s_m}_{m=1}^M
\quad,
\end{eqnarray}
where the $f_m$ are object fluxes, and the $s_m$ are object positions,
shapes, and orientations.  All the fluxes $f_m$ and positions and
shapes $s_m$ are variable parameters.

raster:
\begin{eqnarray}
I(x,y\given\nu) &=& \sum_{k=1}^K a_k\,h_k(x, y)
\\
h_k(x, y) &\equiv& h([x-x_k], [y-y_k])
\\
\theta &\equiv& \set{a_k}_{k=1}^K
\quad,
\end{eqnarray}
where the $a_k$ are raster-element (``pixel'') intensities.  All the
intensities $a_k$ are variable parameters but the element positions
$(x_k, y_k)$ are seen as prior information.

hybrid:
\begin{eqnarray}
I(x,y\given\nu) &=& \sum_{m=1}^M f_m\,g(x,y\given s_m)
                  + \sum_{k=1}^K a_k\,h_k(x, y)
\\
\theta &\equiv& \set{\set{f_m, s_m}_{m=1}^M, \set{a_k}_{k=1}^K}
\quad.
\end{eqnarray}

\paragraph{Intensity priors or regularization}

...Stefan thinks this might not be necessary.  Fergus and Schuler
think this is absolutely necessary...

\end{document}
