% This file is part of the Radio Array Model project.
% Copyright 2012 David W. Hogg

\documentclass[12pt]{article}

\newcounter{hogg}
\setcounter{hogg}{1}
\newcommand{\hoggitem}{\textsl{(\thehogg)}\stepcounter{hogg}}

\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\CLEAN}{\project{CLEAN}}
\newcommand{\NAELC}{\project{NAELC}}

\renewcommand{\exp}[1]{e^{#1}}
\newcommand{\sinc}{\mathrm{sinc}}
\newcommand{\set}[1]{\left\{{#1}\right\}}
\newcommand{\given}{\,|\,}
\newcommand{\expectation}[1]{\tilde{#1}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\normal}{N}
\newcommand{\Normal}{\normal}
\newcommand{\transpose}[1]{{#1}^{\!\mathsf{T}}}

\begin{document}

\textsl{This document is a work in progress being written by
  David~W.~Hogg (NYU).  It is based in part on conversations in
  Valchava on 2012-08-16 among Fergus, Harmeling, Hirsch, Hogg,
  Muandet, Rix, Sch\"olkopf, Schuler and in part on coversations in
  Heidelberg on 2012-08-08 among Bigiel, Herbst, Hogg, Rix, Walter.}

\paragraph{Introduction}

...Interferometry data have these properties...positive reasons to do
what we are doing...

...Define visibility, clean beam, dirty beam; also units and
dimensions...

...For reconstructing a ``best'' image from radio interferometry data,
he radio world uses \CLEAN--—and an infinite variety of alternatives
and modifications thereto—--for building radio interferometry maps...

\paragraph{What's wrong with \CLEAN?}

There are many reasons that the traditional \CLEAN\ procedure needs
replacement or improvement.  Some of these issues have been addressed
in small modifications to the traditional method, leading to a zoo of
baby-\CLEAN s.  The comments below apply only to the most vanilla or
traditional implementations of \CLEAN, though we know of no widely
used system that addresses all of these.

\hoggitem~\CLEAN\ is some kind of optimizer, but a heuristic
optimizer.  That is, there are optimizers out there with better
properties.  \hoggitem~It is also not optimizing a well-specified (let
alone justified) scalar objective.  Fergus said ``a conservative
greedy algorithm'' but with an unknown objective.  Related to this,
\hoggitem~it requires heuristic decision-making about stopping.  There
is a sense of things being over-cleaned or under.

\hoggitem~The noise in the final map \CLEAN\ makes can only be
estimated by measuring variance-like statistics in empty regions.  It
has no error or noise model and doesn't propagate uncertainties from
the fundamental visibilities (let alone calibration uncertainties and
so on).  This blindness to likelihood or noise leads to the problem
that \hoggitem~\CLEAN\ can provide no mechanism for producing or
quantitatively comparing alternative maps.  It produces only a single
point estimate and no judgement of that estimate or sampling around
that estimate.  It is a procedure rather than a probabilistic
framework.  \hoggitem~There is also no mechanism by which the map it
produces is standardly or by convention projected back into the space
of visibilities and compared to the raw data to vet or confirm any
fundamental or even heuristic noise model, or to judge the
significance of faint sources in the maps it produces.

The final map made by \CLEAN\ is a sum of the clean flux (convolved
with the clean beam) and the residual flux (which is effectively
convolved with the dirty beam).  Therefore, \hoggitem~the final map it
makes has no consistent point-spread function: Some of the flux is
convolved with the clean beam, and some is convolved with the dirty
beam.  This means that there is no well-defined point-spread function
and, perhaps more importantly \hoggitem~the absolute calibration of
the map is ill-defined.  The units of the map are ``Jy per clean
beam'' for the bright parts and ``Jy per dirty beam'' for the faint
parts.

On a more technical level (but equally concerning), \hoggitem~\CLEAN\ 
requires binning of the visibility data in the $u$--$v$ plane
(presumably for fastness).  Rix teaches us that ``binning is
sinning''.

\paragraph{Radio interferometric likelihood}

We get from the telescope \emph{visibilities}, which for our purposes
are $N$ samples $d_n$ of the form
\begin{eqnarray}
d_n &\equiv& \set{a_n, b_n, \nu_n, t_n, u_n, v_n, w_n, Z_n}
\\
Z_n &\equiv& A_n + i\,B_n
\\
Z_n &\equiv& \left|Z_n\right|\,\exp{i\,\phi_n}
\quad,
\end{eqnarray}
where $a_n$ and $b_n$ are the identities of the two antennae used to
make the baseline used for sample $n$, $\nu_n$ is the electromagnetic
frequency channel of the observation, $t_n$ is the time at which the
sample was taken, $(u_n, v_n)$ is the $u$--$v$-plane location of the
sample, $w_n$ can be ignored (for our purposes), and $Z_n$ is the
measured complex amplitude or \emph{visibility}.  In what follows, we
will assume that every aspect of the data is correctly calibrated.
Under this assumption, the $Z_n$ are really the data; the $(u_n,
v_n)$ and $\nu_n$ can be thought of as prior information.

Each sample comes from a baseline connecting two antennae; each
antenna has a beam pattern on the sky---what would be the point-spread
function for a single-dish observation---and a system temperature.
There is also a correlator temperature of some kind, so we expect a
noise variance $\sigma_n^2$ on the complex $Z_n$
\begin{eqnarray}
\sigma_n^2 &\propto& T_a + T_b + T_{ab}
\quad ,
\end{eqnarray}
where $T_a$ is the noise temperature of antenna $a_n$, $T_b$ is the
noise temperature of antenna $b_n$, and $T_{ab}$ is an additional
correlator noise for this baseline.

The generative model for our purposes is as follows: There is a
time-independent intensity field $I(l,m\given\nu)$ as a function of
electromagnetic frequency $\nu$, defined in sky coordinates $(l,m)$ or
the $l$--$m$ plane.  These sky coordinates $(l, m)$ are to be thought
of as direction cosines perpendicular to the central line of sight of
the array.  At each $(u_n, v_n)$ sample location in the $u$--$v$ plane
the expected complex visibility is
\begin{eqnarray}
\label{eq:visibility}
\expectation{Z}_n &=& \int \psi_a(l,m\given\nu_n)\,\psi_b(l,m\given\nu_n)\,I(l,m\given\nu_n)
                         \,\exp{-2\pi\,i\,[u_n\,l + v_n\,m]}\,\dd l\,\dd m
\quad ,
\end{eqnarray}
where $\psi_a(l,m\given\nu_n)$ is the single-antenna beam pattern
(proportional to the square-root of the effective area) for antenna
$a$ at frequency $\nu_n$, and $\psi_b()$ is the same for antenna $b$.
This expression (\ref{eq:visibility}) assumes that the electromagnetic
frequency bandpass $\Delta\nu_n$ is small; when the bandpass is
appreciable, there are $\sinc()$ or $\sinc()$-like functions of
products of the bandpass $\Delta\nu_n$ (a frequency width) and the
antenna--antenna delay (a time).  This antenna--antenna delay has both
geometric and delay-line components (that is, the delays between
telescopes are usually adjusted with delay lines).
Expression~(\ref{eq:visibility}) also assumes that the relevant
intensity is close to the central line-of-sight of the array; when the
antenna beam pattern or relevant intensity spans a large angle on the
sky, the solid-angle differential $\dd^2\Omega$ becomes
\begin{eqnarray}
\dd^2\Omega \equiv \frac{\dd l\,\dd m}{\sqrt{1-l^2-m^2}}
\quad ,
\end{eqnarray}
where $l$ and $m$ are thought of as being in their native units, which
are the dimensionless outputs of cosine functions.

In expression~(\ref{eq:visibility}), $\psi_a()$ and $\psi_b()$ are the
square-roots of what is usually thought of as the single-beam
point-spread functions.  For reference, equation~(\ref{eq:visibility})
is the analog of equation~(5.21) in Burke \& Graham-Smith (1997) but
where they use $\cal{A}(l,m)$ for our product of $\psi()$ functions,
and they include the geometric correction $\sqrt{1-l^2-m^2}$.

If the intensity field $I(l,m\given\nu)$ is governed by (a vector,
list, or blob of) parameters $\theta$, then under the Gaussian-noise
assumption there is a likelihood for those parameters
\begin{eqnarray}
p(Z_n|\theta) &=& \normal(Z_n\given\expectation{Z}_n,\sigma_n^2)
\\
-2\,\ln p(Z_n|\theta) &=& \frac{[Z_n - \expectation{Z}_n]^2}{\sigma_n^2}
\\
-2\,\ln p(D|\theta) &=& \sum_{n=1}^N \frac{[Z_n - \expectation{Z}_n]^2}{\sigma_n^2}
\\
D &\equiv& \set{Z_n}_{n=1}^N
\quad ,
\end{eqnarray}
where $\normal(x\given m,V)$ is the Gaussian distribution for $x$
given mean $m$ and variance $V$, we have ignored constant offsets in
the logarithms, we have assembled all the $Z_n$ into a large data blob
$D$, and we have assumed that the noise contributions to the $Z_n$ are
independent.  The likelihood is effectively conditioned on an
assumption that $(u_n, v_n)$ and $\nu_n$---and our knowledge about
beam patterns and noise and so on---are reliable.

\paragraph{Single-dish data}

...tbd...

\paragraph{Intensity models}

There are two general approaches to modeling the intensity field.  In
the first---the \emph{catalog approach}---the intensity is modeled as
being generated by a finite number of discrete objects, each of which
has a flux, a celestial position, and some (perhaps trivial)
morphology.  In this approach, all of the object properties are seen
as free parameters, along with (usually) the total number of objects
in the catalog.  In the second---the \emph{raster approach}---the
intensity is modeled by a set of (perhaps trivial) spatially compact
basis functions arranged in a pre-defined grid in celestial
coordinates.  In this approach, the only parameters permitted to vary
are the amplitudes multiplying the basis functions.  In addition we
will consider a \emph{hybrid approach} in which the intensity is
modeled as a sum or mixture of a catalog and a raster, with the
catalog taking care of the bright, compact sources, and the raster
taking care of the low-signal-to-noise and extended intensity.  In
some sense, The traditional \CLEAN\ procedure produces a hybrid
description of the intensity, because the bright sources are
``cleaned'' as point sources and the faint sources are left behind in
the dirty residual.

The catalog model makes the intensity a mixture of $J$ objects
\begin{eqnarray}
I(l,m\given\nu) &=& \sum_{j=1}^J f_j\,g(l,m\given s_j)
\\
\theta &\equiv& \set{f_j, s_j}_{j=1}^J
\quad,
\end{eqnarray}
where the $f_j$ are object fluxes, the function $g(l,m\given s)$ is a
function that can generate a family of sources with different
positions (and shapes), and the $s_j$ are parameter blobs containing
object positions, shapes, and orientations.  All the fluxes $f_j$ and
positions and shapes $s_j$ are variable parameters.  The catalog model
is not a general model---it is limited by the number of objects and
their morphological flexibility---but it is often fast to optimize and
well-matched to the scientific goals of an imaging project.
Optimization will not be convex, in general, because the positional
and shape parameters will usually affect the intensity in a non-linear
way.

The simplest catalog model is a \emph{mixture of delta-functions}, or
\begin{eqnarray}
g(l,m\given s_j) &=& \delta([l-l_j], [m-m_j])
\\
s_j &\equiv& \set{l_j, m_j}
\quad,
\end{eqnarray}
where $\delta()$ is the two-dimensional delta function and $s_j$
contains only positional information.  Another simple model (really a
generalization) is the \emph{mixture of Gaussians}, or
\begin{eqnarray}
g(l,m\given s_j) &=& \normal(\xi\given\mu_j,V_j)
\\
\xi &\equiv& \transpose{[l, m]}
\\
\mu_j &\equiv& \transpose{[l_j, m_j]}
\\
s_j &\equiv& \set{l_j, m_j, V_j}
\quad,
\end{eqnarray}
where $\normal()$ is here the two-dimensional gaussian, $\xi$ and
$\mu_j$ are two-dimensional position vectors, $V_j$ is a $2\times 2$
symmetric positive-definite variance tensor, and $s_j$ thereby
contains both positional and shape information.

The raster model also makes the intensity a mixture of basis
functions, but now we think of having many functions, all identical
except for positional shifts, and on a fixed grid;
\begin{eqnarray}
I(l,m\given\nu) &=& \sum_{k=1}^K a_k\,h_k(l,m)
\\
h_k(l,m) &\equiv& h([l-l_k], [m-m_k])
\\
\theta &\equiv& \set{a_k}_{k=1}^K
\quad,
\end{eqnarray}
where the $a_k$ are raster-element (``pixel'') intensities.  All the
intensities $a_k$ are variable parameters but the element positions
$(l_k, m_k)$ are seen as prior information.  The raster model is
``non-parametric'' in that it can represent very strange intensity
fields, but at the cost of large numbers of parameters.  One can hope
to keep things convex.

Again, the simplest pixel functions are delta-functions
\begin{eqnarray}
h_k(l,m) &=& \delta([l-l_k], [m-m_k])
\quad,
\end{eqnarray}
but there are many other options, including square top-hat functions
and circular Gaussians (HOGG put in formulae).

...perhaps note here that in the raster model there is some hope for
linear least squares; be explicit about the linear algebra...

The hybrid method is more general, being a mixture of the two:
\begin{eqnarray}
I(l,m\given\nu) &=& \sum_{j=1}^J f_j\,g(l,m\given s_j)
                  + \sum_{k=1}^K a_k\,h_k(l,m)
\\
\theta &\equiv& \set{\set{f_j, s_j}_{j=1}^J, \set{a_k}_{k=1}^K}
\quad.
\end{eqnarray}
Optimization will be difficult because it has all the parameters of
both models, and all the non-convexity of the catalog method.  Because
traditional \CLEAN\ works by modeling parts of the intensity field as
point sources, and parts as a dirty residual map, it produces, in some
sense, a hybrid model of the intensity field.  It is not, however, an
optimal map in any sense, and it does not have consistent beam-shape
(PSF) properties, as noted above.

One oddity of all of these intensity models is that they are models of
the \emph{unconvolved} or \emph{infinite-resolution} intensity field.
That object not only doesn't exist (in Hogg's opinion) but is
unobservable and uncomparable to any other observation.  For this
reason, and to avoid ``deconvolution artifacts'' of various kinds,
radio astronomers typically present these intensity models back to the
community only after re-convolving them with a \emph{clean beam}.  The
clean beam is some kind of simple (Gaussian, perhaps) beam that has
the same FWHM properties as the original dirty beam.

\paragraph{Intensity priors or regularization}

For any model of the intensity field---catalog, raster, or
hybrid---there is a large parameter vector $\theta$.  In general we
have \emph{priors} on this parameter vector that indicate the behavior
we would like for our inference in the absence of data or when the
data are not informative.  Because we are producing point estimates,
these things we call ``priors'' really involve not just our prior
expectations about the plausibility of any scene but also our
\emph{utility} or preferences.

...Harmeling thinks this might not be necessary.  Fergus and Schuler
think this is absolutely necessary...

...non-negativity...

...spike and slab...

...smoothness or gradients...

\paragraph{Initialization and optimization}

...Schuler will save us all?...

...Do we have tricks that make our inferences fast?  Certainly we know
that, for the raster components, the major operations are linear
operations and contain some identical linear-algebra objects at every
step...

\end{document}
